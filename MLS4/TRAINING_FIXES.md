# è®­ç»ƒå‡†ç¡®ç‡ä¿®å¤è¯´æ˜

## æœ€æ–°ä¿®å¤ (2025-11-16)

### ğŸ”§ ä¿®å¤ `sample_weight` TypeError

**é—®é¢˜ç—‡çŠ¶:**
```
TypeError: Reduce.update_state() got multiple values for argument 'sample_weight'
```

**æ ¹æœ¬åŸå› :**
- `tf_keras` åœ¨ä½¿ç”¨ `tf.data.Dataset.from_tensor_slices((x, y)).batch()` åˆ›å»ºçš„æ•°æ®é›†æ—¶
- åœ¨ `model.evaluate()` è¿‡ç¨‹ä¸­ä¼šé”™è¯¯åœ°é‡å¤ä¼ é€’ `sample_weight` å‚æ•°ç»™ metrics

**è§£å†³æ–¹æ¡ˆ:**

1. **ä¿®æ”¹æ•°æ®é›†åˆ›å»º (main.py)**:
```python
def to_dataset(x, y, batch_size):
    """Create TF dataset with proper configuration to avoid sample_weight conflicts."""
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.batch(batch_size, drop_remainder=False)
    dataset = dataset.prefetch(AUTOTUNE)
    return dataset
```

2. **æ·»åŠ è¯„ä¼°é”™è¯¯å¤„ç† (part2_quantization.py)**:
```python
def _evaluate_model(self, model, dataset):
    try:
        metrics = model.evaluate(dataset, verbose=0)
        ...
    except TypeError as e:
        if "sample_weight" in str(e):
            # Fallback: manual evaluation
            total_correct = 0
            total_samples = 0
            for x_batch, y_batch in dataset:
                predictions = model.predict(x_batch, verbose=0)
                ...
            return float(total_correct / total_samples)
        raise
```

**éªŒè¯ä¿®å¤:**
```bash
python main.py --batch-size 128
```

---

## é—®é¢˜è¯Šæ–­

ä»æ‚¨æä¾›çš„è®­ç»ƒæ—¥å¿—æ¥çœ‹,å­˜åœ¨ä¸¥é‡çš„**è¿‡æ‹Ÿåˆ**é—®é¢˜:
- è®­ç»ƒé›†å‡†ç¡®ç‡ï¼šæŒç»­ä¸Šå‡è‡³ 76.84%
- éªŒè¯é›†å‡†ç¡®ç‡ï¼šåœæ»åœ¨ 5-7% å·¦å³ï¼ˆæ¥è¿‘éšæœºçŒœæµ‹ 1/100 = 1%ï¼‰
- éªŒè¯æŸå¤±ï¼šä¸æ–­å¢å¤§ï¼ˆä» 6.06 â†’ 7.98ï¼‰

è¿™è¡¨æ˜æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šå­¦ä¹ è¿‡åº¦ï¼Œä½†å®Œå…¨æ— æ³•æ³›åŒ–åˆ°éªŒè¯é›†ã€‚

## å·²å®æ–½çš„ä¿®å¤

### 1. **é™ä½å­¦ä¹ ç‡** âœ…
```python
# ä¿®æ”¹å‰: learning_rate=1e-3
# ä¿®æ”¹å: learning_rate=5e-4
optimizer = tf.keras.optimizers.AdamW(learning_rate=5e-4, weight_decay=5e-5)
```
- æ›´ä¿å®ˆçš„å­¦ä¹ ç‡é¿å…è®­ç»ƒé›†ä¸Šè¿‡å¿«æ”¶æ•›
- é™ä½æƒé‡è¡°å‡ç³»æ•° (1e-4 â†’ 5e-5) å‡å°‘æ­£åˆ™åŒ–å‹åŠ›

### 2. **å¢åŠ Label Smoothing** âœ…
```python
# ä¿®æ”¹å‰: label_smoothing=0.05
# ä¿®æ”¹å: label_smoothing=0.1
loss=AdaptiveCategoricalCrossentropy(num_classes=num_classes, label_smoothing=0.1)
```
- æ›´å¼ºçš„æ ‡ç­¾å¹³æ»‘é˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ ·æœ¬è¿‡åº¦è‡ªä¿¡

### 3. **ç®€åŒ–æ•°æ®å¢å¼º** âœ…
```python
# ç§»é™¤äº†è¿‡åº¦çš„å¢å¼ºæ“ä½œ:
# âŒ RandomZoom
# âŒ RandomContrast
# âœ… ä¿ç•™: RandomFlip, RandomTranslation, RandomRotation (é™ä½å¼ºåº¦)
```
- å‡å°‘äº†å¯èƒ½å¯¼è‡´è®­ç»ƒ/éªŒè¯ä¸ä¸€è‡´çš„å¢å¼ºæ“ä½œ

### 4. **å¢åŠ Dropout** âœ…
```python
# ä¿®æ”¹å‰: dropout_rate=0.2
# ä¿®æ”¹å: dropout_rate=0.3
```
- æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

### 5. **ä¼˜åŒ–å­¦ä¹ ç‡è°ƒåº¦** âœ…
- æ·»åŠ äº† **5 epoch warmup** é˜¶æ®µ
- è°ƒæ•´ ReduceLROnPlateau:
  - ç›‘æ§æŒ‡æ ‡: `val_accuracy` â†’ `val_loss`
  - patience: 5 â†’ 3 (æ›´å¿«å“åº”éªŒè¯é›†æ€§èƒ½ä¸‹é™)
  - min_lr: 1e-5 â†’ 1e-6

### 6. **è°ƒæ•´Early Stopping** âœ…
```python
# ç›‘æ§ val_loss è€Œé val_accuracy
# patience ä» 10 å¢åŠ åˆ° 12
```

### 7. **å¢åŠ æ‰¹æ¬¡å¤§å°** âœ…
```python
# ä¿®æ”¹å‰: batch_size=128
# ä¿®æ”¹å: batch_size=256
```
- æ›´å¤§çš„æ‰¹æ¬¡æä¾›æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
- å‡å°‘è®­ç»ƒå™ªå£°

### 8. **å»¶é•¿è®­ç»ƒå‘¨æœŸ** âœ…
```python
# ä¿®æ”¹å‰: epochs=30
# ä¿®æ”¹å: epochs=50
```
- ç»™æ¨¡å‹æ›´å¤šæ—¶é—´ä»¥è¾ƒä½å­¦ä¹ ç‡æ”¶æ•›

## éªŒè¯å’Œæµ‹è¯•

### è¿è¡Œæ•°æ®éªŒè¯è„šæœ¬
```bash
python verify_data.py
```

æ­¤è„šæœ¬ä¼šæ£€æŸ¥ï¼š
- âœ… è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†å½’ä¸€åŒ–æ˜¯å¦æ­£ç¡®
- âœ… æ ‡ç­¾åˆ†å¸ƒæ˜¯å¦å‡åŒ€
- âœ… æ•°æ®é›†ä¹‹é—´æ²¡æœ‰é‡å 
- âœ… æ•°æ®ç®¡é“æ­£ç¡®é…ç½®ï¼ˆéªŒè¯é›†æ— å¢å¼ºï¼‰
- âœ… æ¨¡å‹å¯ä»¥æ­£å¸¸æ¨ç†

### é‡æ–°è®­ç»ƒæ¨¡å‹
```bash
# åˆ é™¤æ—§çš„checkpoint
rm -rf checkpoints/baseline_best.keras baseline_model.keras

# é‡æ–°è®­ç»ƒ
python main.py --train-baseline --batch-size 256
```

## é¢„æœŸç»“æœ

ä¿®å¤åï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ï¼š
- âœ… éªŒè¯é›†å‡†ç¡®ç‡ç¨³æ­¥ä¸Šå‡ï¼ˆç›®æ ‡ >40% @ epoch 30ï¼‰
- âœ… è®­ç»ƒ/éªŒè¯å‡†ç¡®ç‡å·®è·ç¼©å°ï¼ˆ<15%ï¼‰
- âœ… éªŒè¯æŸå¤±å…ˆä¸‹é™å†è¶‹äºç¨³å®š
- âœ… Top-5 å‡†ç¡®ç‡ >70%

å…¸å‹çš„å¥åº·è®­ç»ƒæ›²çº¿ï¼š
```
Epoch 10: train_acc=0.35, val_acc=0.28, val_loss=2.8
Epoch 20: train_acc=0.55, val_acc=0.48, val_loss=2.1
Epoch 30: train_acc=0.68, val_acc=0.58, val_loss=1.8
Epoch 40: train_acc=0.75, val_acc=0.65, val_loss=1.6
```

## å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨

å¦‚æœéªŒè¯å‡†ç¡®ç‡ä»ç„¶å¾ˆä½ï¼Œè¯·æ£€æŸ¥ï¼š

### 1. æ•°æ®åŠ è½½é—®é¢˜
```python
# åœ¨ baseline_model.py ä¸­æ·»åŠ è°ƒè¯•ä»£ç 
print(f"Train labels unique: {np.unique(y_train)}")
print(f"Val labels unique: {np.unique(y_val)}")
```

### 2. æ ‡ç­¾ç¼–ç é—®é¢˜
```python
# ç¡®è®¤ one-hot ç¼–ç æ­£ç¡®
for x, y in val_ds.take(1):
    print(f"Label shape: {y.shape}")  # åº”è¯¥æ˜¯ (batch_size, 100)
    print(f"Label sum: {tf.reduce_sum(y, axis=-1)}")  # åº”è¯¥å…¨æ˜¯ 1.0
```

### 3. å°è¯•æ›´ç®€å•çš„æ¨¡å‹
```python
# æš‚æ—¶å‡å°‘æ¨¡å‹å¤æ‚åº¦æµ‹è¯•
model = create_baseline_model(width_multiplier=0.5, dropout_rate=0.4)
```

## è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

å¦‚æœåŸºç¡€è®­ç»ƒç¨³å®šåï¼Œå¯ä»¥è€ƒè™‘ï¼š
1. **Mixup/CutMix** æ•°æ®å¢å¼º
2. **Cosine Annealing** å­¦ä¹ ç‡è°ƒåº¦
3. **Gradient Clipping** (clip_norm=1.0)
4. **SAM optimizer** (Sharpness Aware Minimization)
5. **Test-Time Augmentation**

## å‚è€ƒ

- [CIFAR-100 SOTA](https://paperswithcode.com/sota/image-classification-on-cifar-100)
- [Label Smoothing Paper](https://arxiv.org/abs/1512.00567)
- [Bag of Tricks for Image Classification](https://arxiv.org/abs/1812.01187)
